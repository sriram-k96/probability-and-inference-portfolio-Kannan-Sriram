---
title: "99-final-exam"
author: ' Sriram Kannan'
output:
  html_document:
    code_folding: hide
    toc: yes
    number_sections: yes
    toc_depth: 3
    toc_float: yes
---


# 1. Simulation

The Monte Hall problem is a classic game show.  Contestants on the show where shown three doors.  Behind one randomly selected door was a sportscar; behind the other doors were goats.

At the start of the game, contestants would select a door, say door A.  Then, the host would open either door B or C to reveal a goat.  At that point in the game, the host would ask the contestant if she would like to change her door selection.  Once a contestant decided to stay or change, the host would open the chosen door to reveal the game prize, either a goat or a car.

In this problem, consider a **modified** version of the Monte Hall problem in which the number of doors is **variable**.  Rather than 3 doors, consider a game with 4 or 5 or 50 doors.  In the modified version of the game, a contestant would select an initial door, say door A.  Then, the host would open **one** of the remaining doors to reveal a goat.  At that point in the game, the host would ask the contestant if she would like to change her door selection.  Once a contestant decided to stay or change, the host would open the chosen door to reveal the game prize, either a goat or a car.

Consider two strategies:
  
  1. Always stay with the first door selected.
  2. Always switch to the unopened door.

**C.** The function `game` below plays a single game of Monte Hall.  The function returns a vector of length two, the first element is the prize under strategy 1 and the second element is the prize under strategy 2.  The function has a single input parameter, N, which is the number of doors in the game.

Use the `game` function to estimate the probability that both strategies result in a goat. Let **N=4**.
```{r}
require(magrittr)
require(dplyr)

game <- function(N){
  if(N<3) stop("Must have at least 3 doors")
  #1 car, everything else is a goat. 
  prize <- sample(c(rep("goat",N-1),"car"), N)
  guess <- sample(1:N,1)
  game <- data.frame(door = 1:N, prize = prize, stringsAsFactors = FALSE) %>% 
    mutate(first_guess = case_when(
      door == guess ~ 1
      , TRUE ~ 0
    )) %>% 
    mutate(potential_reveal = case_when(
        first_guess == 1 ~ 0
      , prize == "car" ~ 0
      , TRUE ~ 1
    )) %>% 
    mutate(reveal = 1*(rank(potential_reveal, ties.method = "random") == 3)) %>% 
    mutate(potential_switch = case_when(
      first_guess == 1 ~ 0
      , reveal == 1 ~ 0
      , TRUE ~ 1
    )) %>% 
    mutate(switch = 1*(rank(potential_switch, ties.method = "random") == 3))
  c(game$prize[game$first_guess == 1], game$prize[game$switch == 1])
}
```

```{r}
g1 = c()
l1 = c()
#co = c()

for (i in 1:5000){
  co = game(4)
  g1[i] <- co[1]
  l1[i] <- co[2]
}
```

```{r}
#Probability for Both Strategies result in goat = Probability Strat 1 is goat * Prob that Strat 2 is goat
#Since Strat 1 and 2 are not being done in a sequence, they are disjoint. 
length(g1[g1 == "goat"])/length(g1) * length(l1[l1 == "goat"])/length(l1)
```
**B**. Communicate the precision of your simulated probability in part **C** by calculating a **99\%** confidence interval.

```{r}
# g = c()
# l = c()
# m1 = c()
# m1mean = c()
# 
# for (k in 1:50)
# {
#   for (j in 1:50)
#     {
#     for (i in 1:50){
#       co = game(4)
#       g[i] <- co[1]
#       l[i] <- co[2]
#     }
#     m1[j] <- length(g[g == "goat"])/length(g) * length(l[l == "goat"])/length(l)
#   }
#   m1mean[k] <- m1 %>% mean
# }
# 
# quantile(m1mean, c(0.005, 0.995))
```

```{r}
prop.test(2325, 5000, conf.level = 0.99)
```
A. Let D(N) be the difference between the difference in probabilities between strategy 2 and strategy 1.

D(N)=P(win strategy 2|N doors)−P(win strategy 1|N doors)
Create a plot that shows how D changes as N increases. Put N on the x-asis, ranging from 3 to 10. Put D on the y-axis.


```{r}
g1 = c()
l1 = c()
r1 = c()

for (k in 3:10)
  {
    for (i in 1:5000){
    co = game(k)  
    g1[i] <- co[1]
    l1[i] <- co[2]
    }
  r1[k] = length(l1[l1 == "goat"])/length(l1) - length(g1[g1 == "goat"])/length(g1)
}

```

```{r}
N = seq(from = 3, to = 10, by = 1)
plot(N, r1[3:10], xlab = "N", ylab = "D(N)")
```

2. Probability
Consider a test for a rare genetic condition. Let T+ denote a test result that indicates the condition is present, while T- denotes absence. Let D+ and D- denote the true status of the disease.

C. Fill-in the probability table using the following information:

P(T+|D+) = .85, and
P(T-|D-) = .95, and
P(D+) = 0.001

!(C:\Users\lucid\Documents\Projects\Vandy Assignments\Probability and Statistical Inference\probability-and-inference-portfolio-Kannan-Sriram\99-final-exam)

B. Calculate the negative predictive value of the test, P(D-|T-).

P(D-|T-) = 0.9998

A Create a plot that shows how the positive predictive value as a function of the prevalence of disease, P(D+).

PPV = P(D+|T+) = P(T+ & D+)/P(T+) = P(T+|D+) * P(D+)/P(T+)
```{r}
prevalence <- seq(from = 0.001, to = 0.1, length.out =  50)
ppv <- (0.85*prevalence)/0.05
#ppv
plot(prevalence, ppv, xlab = "Prevalence", ylab = "PPV")
```


3. Discrete Distributions
Suppose the yearly hospital charges (in thousands of dollars) for a randomly selected Vanderbilt student is a mixture distribution.

For 50% of students, the hospital charges will be $0. For the remaining 50% of students, the hospital charges are a random variable described by a gamma distribution with shape = 2 and scale = 2. (Again, in thousands of dollars.) 
```{r}
hospital_charges <- function(N){
  group <- rbinom(N, 1, 0.5)
  #1 - group takes the complement of the output from the binom fn. Thus half the students that aren't flagged for 0 bills are given the gamma distribution while the other half are charged 0 based on the first term of the eqn. 
  charges <- 0*group + rgamma(N, shape = 2, scale = 2)*(1-group)
  charges
}

```

C. What is the 90th percentile for yearly hospital charges for a randomly selected Vanderbilt student?

```{r}
q <- hospital_charges(10000)  
quantile(q, 0.9)
```


B. Consider the class average yearly hospital charge for the students in a class of size 30. Plot the density function or a simulated histogram of the class average yearly hospital charge.

```{r}
ca <- c()
for (i in 1:100000){
  ca[i] <- hospital_charges(30) %>% mean }
hist(ca, freq = FALSE, xlab = "Class Averge Yearly Hospital Charge in Thousands of $", main = "Histogram of Class Average Yearly Hospital Charge")
```

A. What is the probability that a randomly selected class of size 30 students will have less than 10 students with zero yearly hospital charges?

```{r}
b = c()
for (i in 1:100000)
{
  a <- hospital_charges(30)
  #Counting 0 values in a and comparing
  b[i] <- length(a[a == 0]) < 10
}
#b*1 is to turn Boolean to Int (0,1)
mean(b*1)
```

4. Continuous Distributions
C. Suppose diastolic blood pressure (DBP) follows a normal distribution with mean 80 mmHg and SD 15 mmHg. What is the probability that a randomly sampled person’s DBP lies between 70 and 104 mmHg?

```{r}
pnorm(104, mean = 80, sd = 15) - pnorm(70, mean = 80, sd = 15) 

```

B. Suppose a human femur was discovered that is 37 cm long. Also suppose that using the NHANES data, researchers believe the distribution of femur bones, by sex, are distributed as follows:

Female adult femur ∼N(36,3.3)
Male adult femur ∼N(40,3.4)
Under the assumption that male and females are equally likely, what is the probability that the discovered femur was from a male?

```{r}
#Bayes Rule : P(Male|Femur) = P(Femur|Male) * P(Male)/P(Femur|Male) + P(Femur|Female)

(dnorm(37, mean = 40, sd = 3.4) * 0.5) / (dnorm(37, mean = 36, sd = 3.3) + dnorm(37, mean = 40, sd = 3.4))
```


A. Continuing part B, generate a plot of P(femur from male | femur length = x). Let femur length range from 25 to 50.

P(femur from male | femur length = x) = P(femur length = x | femur from male) * P(femur from male)/P(femur length = x)

```{r}
femur_length <- seq(from = 25, to = 50)
prob_male = c()
for (i in femur_length)
{
  prob_male[i] = (dnorm(i, mean = 40, sd = 3.4) * 0.5) / (dnorm(i, mean = 36, sd = 3.3) + dnorm(i, mean = 40, sd = 3.4))
}
prob_male <- prob_male[is.na(prob_male) == FALSE]
#prob_male <- (dnorm(37, mean = 40, sd = 3.4) * 0.5) / (dnorm(37, mean = 36, sd = 3.3) + dnorm(37, mean = #40, sd = 3.4))
plot.new()
plot.window(xlim = c(25,50), ylim = c(0,1))
lines(femur_length, prob_male)
axis(1)
axis(2)
box()
title(xlab = "Femur Length", ylab = "P(Male | femur length)")
```

5. Expectation and Variance
Let us revisit the yearly hospital charges distribution from a previous section.

Recall: The yearly hospital charges (in thousands of dollars) for a randomly selected Vanderbilt student is a mixture distribution. For 50% of students, the hospital charges will be $0. For the remaining 50% of students, the hospital charges are a random variable described by a gamma distribution with shape = 2 and scale = 2. (Again, in thousands of dollars.)

```{r}
hospital_charges <- function(N){
  group <- rbinom(N, 1, 0.5)
  charges <- 0*group + rgamma(N, shape = 2, scale = 2)*(1-group)
  charges
}
```

C. What is E[yearly hospital charges]?

```{r}
#E[X] = Mean
#Half the Students have 0 fee, rest have gamma dist. Hence we find expectation of gamma dist * 0.5 = shape * scale / 2

shape = 2
scale = 2
p = 0.5

scale*scale*p

hospital_charges(100000) %>% mean #- Simulation to test it

```

B. Suppose Vanderbilt implements a cap of $10,000 on yearly student hospital charges. What is the mean yearly hospital charge under the new policy?

```{r}
#Mean is around 1.95
#Using a high sample size to simulate
a <- hospital_charges(1000000)
#Resetting any values above 10 to 10
a[a > 10] = 10
a %>% mean
```

A. What is the variance of yearly hospital charge under the new policy?

```{r}
#Var is around 7
a %>% var
```


6. Transformations & Sampling Distributions
C. Consider the log normal distribution. If X is a log normal random variable, then log(X) is a normal random variable. One way to create pseudo-random draws from the log normal distribution is to generate draws from a normal distribution and then to transform the draws by exponentiating. The parameters of the log normal distribution are the parameters of the underlying normal distribution, μ and σ (or σ2).

Log normal data are prevalent is biological systems and econometrics.

Suppose a blood chemistry measure has a log normal distribution with μ = 0 and σ = 1. Generate an histogram or density curve for the sampling distribution of the median when the sample size is 101.

```{r}
#Based on desc. creating log normal random draws from underlying normal dist. 
c <- c()
for (i in 1:5000)
{
  c[i] <- exp(rnorm(101, mean = 0, sd = 1)) %>% median
}
hist(c, xlab = "Median", main = "Sampling distribution of standard lognormal dist.", breaks = 10)
```


B. Below is the CDF function for the kth order statistic when the underlying distribution is log normal with μ = 0 and σ = 1. Create a plot of the ECDF of the simulated sampling distribution generated in C and overlay the CDF using the function below.

```{r}
Fk <- function(x,k,n){
  pbinom(k-1, n, plnorm(x), lower.tail = FALSE)
}

plot(ecdf(c), xlab = "median", ylab = "Probability", main = "ECDF vs Order Statistic for Sampling Dist of Median")
curve(Fk(x, 51, 101), from = 0.4, to = 2, add = TRUE, col = "red")
```

A. Of the 25th, 50th, and 75th quantiles of the distribution from B, which will have the tightest 95% CI? (Show the sampling distribution of each.)

Difference between 97.5% and 2.5% quantile for sampling dist of below quantiles : 
X25 - 0.249
X50 - 0.446
X75 - 0.932

Thus X25 has the tightest 95% CI. 
```{r}

porder <- function(x,k,n){
  pbinom(k-1, n, plnorm(x, 0, 1), lower.tail = FALSE)
}

qorder <- function(p, k, n){
  output <- NA
  for (i in seq_along(p)) {
    f <- function(x,p){
    return(porder(x)-p[i])
    }
    output[i] <- uniroot(function(x) {porder(x, k, n)-p[i]}, c(-100, 100))$root
  }
  return(output)
}
a <- seq(0.4, 2, 0.001)

p <- seq(.01, .99, by = .01)

plot(p, qorder(p, 25, 101), xlab = "Probability", ylab = parse(text="X[(25)]"))
plot(p, qorder(p, 50, 101), xlab = "Probability", ylab = parse(text="X[(50)]"))
plot(p, qorder(p, 75, 101), xlab = "Probability", ylab = parse(text="X[(75)]"))

ci1 <- quantile(qorder(p,25,101), c(0.025, 0.975))
ci2 <- quantile(qorder(p,50,101), c(0.025, 0.975))
ci3 <- quantile(qorder(p,75,101), c(0.025, 0.975))
ci1
ci2
ci3

```


7. Estimation of CDF and PDF from data

The following code will load the NHANES data and select the first 500 rows.

```{r}
Hmisc::getHdata(nhgh)
d1 <- nhgh[1:500,]
```

C. Estimate the distribution of standing height for adult (age > 18) males using the MLE method with a normal distribution. Create a plot of the estimated density function.

```{r}
htma <- d1 %>% filter(sex == "male", age > 18) 
ht <- htma$ht


nLLht <- function(mean, sd){
  fs <- dnorm(
        x = ht
      , mean = mean
      , sd = sd
      , log = TRUE #We're trying to maximize log likelihood
    ) 
  -sum(fs) #Negative log Likelihood - the mle function below needs this as the first argument
  #ht is a sample with multiple values. We multiply each of the likelihood fns for every individual sample.
  #Since Log Likelihood is used, we add them instead. 
}

fitnormht <- mle(
    nLLht
  , start = list(mean = 0, sd = 1) #Initial values for Starting values to maximize the log likelihood. Can be taken from the -log likelihood fn above but we have not initialized it there. 
  , method = "L-BFGS-B" #Optimization method that allows for constraints
  , lower = c(0, 0.01)
)


hist(ht, breaks = 100, freq = FALSE, main = "PDF of Height of Adult Males - Normal Dist.")
curve(
    dnorm(x, mean = coef(fitnormht)[1], sd = coef(fitnormht)[2])
  , add = TRUE
  , col = "blue"
  , lwd = 3
)
```


B. Estimate the distribution of BMI for adult (age > 18) females using using the method of moment method with the gamma distribution. Create a plot of the estimated density function.

```{r}
htfa <- d1 %>% filter(age > 18, sex == "female")
BMI <- htfa$bmi

xbarBMI <- mean(BMI)
s2BMI <- var(BMI)
#Gamma Dist - Mean = shape*scale (First Moment)
#Variance = shape * (scale)^2 (2nd Moment)

#Solved Simultaneous eqns - Get Gamma Params in terms of sample mean and variances
shape_hatBMI <- xbarBMI^2/s2BMI 
scale_hatBMI <- s2BMI/xbarBMI

fBMIgamma <- function(x){dgamma(x, shape = shape_hatBMI, scale = scale_hatBMI)}

hist(BMI, freq = FALSE, breaks = 100)
curve(fBMIgamma(x), from = 10, to = 100, add = TRUE, col = "blue", lwd = 3)

```


A. Estimate the distribution of creatinine (SCr) for adults (age > 18) using the kernel density method with a Gaussian kernel. Create a plot of the estimated density function.

```{r}
hta <- d1 %>% filter(age > 18)
Scr <- hta$SCr 
Scr
Scr <- Scr[is.na(Scr) == FALSE]
Scr

hist(Scr, freq = FALSE, breaks = 100, main = "PDF of SCr for Adults")
lines(density(Scr, bw = "nrd0", adjust = 1, kernel = "gaussian", na.rm = TRUE), add = TRUE)

#density(Scr, bw = "nrd0", adjust = 1, kernel = "gaussian", na.rm = TRUE)

# epdfstar <- function(t, data, smooth){
#   outer(t, data, function(a,b){ dnorm(a, b, smooth)}) %>% rowMeans
# }
# 
# hist(Scr, freq = FALSE, main = "", breaks = 100)
# curve(epdfstar(x, Scr, smooth = .1), add = TRUE, lwd = 3, col = "orange")
# curve(epdfstar(x, Scr, smooth = .5), add = TRUE, lwd = 3, col = "blue")
# curve(epdfstar(x, Scr, smooth = 1), add = TRUE, lwd = 3, col = "red")
# 
# ecdfstar <- function(t, data, smooth){
#   outer(t, data, function(a,b){ pnorm(a, b, smooth)}) %>% rowMeans
# }
# plot(ecdf(Scr), main = "")
# curve(ecdfstar(x, Scr, smooth = 1), add = TRUE, lwd = 3, col = "red")
# curve(ecdfstar(x, Scr, smooth = 0.5), add = TRUE, lwd = 3, col = "blue")
# curve(ecdfstar(x, Scr, smooth = .1), add = TRUE, lwd = 3, col = "orange")


```


8. Sample from an estimated distribution
The following code will load the low birth weight data from the MASS package. The description of the variables in the dataset can be found in the birthwt documentation with the command ?MASS::birthwt.

```{r}
bwt <- MASS::birthwt

```

C. Generate a 95% confidence interval for the mean birthweight of infants whose mothers did smoke during pregnancy using the bootstrap.

```{r}
nbwt <- bwt %>% 
  filter(smoke == 1) %>% 
  pull(bwt)

n_nbwt <- length(nbwt)
M <- 5000
out <- rep(NA, M)
for(i in 1:M){
  #Bootstrap sampling - Sampling Randomly Sampling 5000 values with replacement - Here we get the indices
  index <- sample.int(n_nbwt, n_nbwt, replace = TRUE)
  out[i] <- nbwt[index] %>% mean
}
quantile(out, c(0.025, 0.975))
```


B. Generate a 95% confidence interval for the mean birthweight of infants whose mothers did smoke during pregnancy using the Central Limit Theorem shortcut.

```{r}
t.test(nbwt)
```

A. Let μs be the mean birthweight of infants whose mothers smoked during pregnancy. Let μns be the mean for the non-smoking group. Use simulation to calculate the 95% confidence interval for μs/μns.

```{r}
nsbwt <- bwt %>% 
  filter(smoke == 0) %>% 
  pull(bwt)

n_nsbwt <- length(nsbwt)
M <- 5000
out2 <- rep(NA, M)
for(i in 1:M){
  index2 <- sample.int(n_nsbwt, n_nsbwt, replace = TRUE)
  out2[i] <- nsbwt[index2] %>% mean
}
quantile(out2, c(0.025, 0.975))

out3 <- out/out2
quantile(out3, c(0.025, 0.975))
```

9. Inference
C. Suppose two studies were performed looking at the risk of mild complication after hernia repair using open and laparoscopic surgical approaches. The study results are below. Using the data from each study individually, perform the hypothesis test that the risk of complication between open and laparoscopic repairs are the same under the usual point null. What is the p-value from each study? What do you conclude from each study?


Study1	Comp	No comp
Open	    30	     70
Lap	      35	     65
Study2	Comp	No comp
Open	    600	   1400
Lap	      619	  1381

H0 = Risk of Complications are the same 
H1 = Risk of Complications are different

Based on the tests performed, we fail reject the null hypothesis due to inconclusive results in both studies as the confidence intervals for the risk of complications found straddle each other in both studies. 
between 
The p-value is less than 2.2e^-16, i.e less than 0.05 in both studies.
```{r}
o1 = prop.test(30,100)
o1
l1 = prop.test(35,100)
l1

d1 = prop.test(5,100)
#d1

o2 = prop.test(600,2000)
o2
l2 = prop.test(619,2000)
l2

d2 = prop.test(19,2000)
#d2
```

B. Suppose that prior to the studies, the researchers established an equivalence threshold of 6 percentage points. Using the confidence intervals, which studies (if any) showed a conclusive similarity between surgical approaches for the complication rate. Explain why.

Study 2 shows conclusive similarity as we get 0.005 and 0.015 as the conf interval which corresponds to 0.5 - 1.5% which is well within the -6% - 6% equivalence threshold. 

Study 1 gives an inconclusive result because we get a conf interval of 0.018 and 0.118 which corespond to 1.8% and 11.8% which straddles the equivalence threshold.
```{r}
d1
d2
```


A. If the data from the studies were combined, what is the smallest equivalence threshold that would identify a conclusive similarity between the surgical approaches?

Smallest equivalence threshold would be 1.73%(approx) (-1.73, 1.73) to identify a conclusive difference
```{r}
o3 <- prop.test(630, 2100)
o3
l3 <- prop.test(654, 2100)
l3

d3 <- prop.test(24,2100)
d3
```


10. Joint Distributions
C. Fill in the blank. The sample correlation is a measure of linear association.

B. Explain why predictions from a conditional distribution generally have smaller prediction error than predictions from the marginal distribution.

Conditional Distributions have lower variances which is directly proportional to the average prediction error. Variance is higher in marginal distribution. This is due to the fact that covariance (that is subtracted from variance) is 0 in case of independence. 

A. Use the CLT shortcut to calculate the 95% confidence interval for the correlation of arm circumference and arm length using the NHANES dataset. Is the sample correlation a reasonable measure of association for this data?

```{r}
Hmisc::getHdata(nhgh)
nhgharm <- nhgh %>% select(armc, arml)
cor.test(nhgharm$armc, nhgharm$arml)
```
Yes, sample correlation is a reasonable measure of association (linear) for this data as the sample correlation falls in the confidence interval estimated by the CLT shortcut. 




